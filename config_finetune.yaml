model:                              # architecture details
    type: resnet18_official         # model name
    kwargs:
        freeze_layer: True
        num_classes: 10            # dimension of features
        bn:
            use_sync_bn: False      # whether to use syncbn
            kwargs: {}


optimizer:                  # optimizer details
    type: LARS
    kwargs:
        nesterov: False
        momentum: 0.9
        weight_decay: 0.0
        implementation: PyTorch

lr_scheduler:                   # learning rate scheduler details
    type: CosineEpoch
    kwargs:
        base_lr: 0.8 # 0.32 #       # initial leaning rate
        warmup_lr: 0.8 # 0.32 #        # learning rate after warming up
        warmup_epoch: 0 #25000         # iterations of warmup
        min_lr: 0.0             # mimimal learning rate
        max_epoch: 90 #28150       # total iterations of training 1281167

lms:                      # large model support: utilize cpu to save gpu memory
    enable: False         # whether to use lms
    kwargs:
        limit: 12         # the soft limit in G-bytes on GPU memory allocated for tensors

data:                     # data details
    type: imagenet        # choices = {'imagenet', 'custom'}
    read_from: fs         # choices = {'mc', 'fs', 'fake', 'osg'}
    
    batch_size: 256         # batch size in one GPU
    num_workers: 16        # number of subprocesses for data loading
    pin_memory: True      # whether to copy Tensors into CUDA pinned memory
    input_size: 224       # training image size         # DIISABLED IN TRANSFORM
    test_resize: 256      # testing resize image size   # DIISABLED IN TRANSFORM

    train:
        root_dir:  /home/liweiquan/data/cifar10/cifar-10-png/train  # Imagenet dataset
        meta_file:  /home/liweiquan/data/cifar10/cifar-10-png/train.txt   # Imagenet dataset meta file
        image_reader:
            type: pil
        sampler:
            type: distributed_epoch #distributed_iteration
        transforms:
            type: LINEAR

    test:
        root_dir:  /home/liweiquan/data/cifar10/cifar-10-png/test  # Imagenet val dataset
        meta_file:  /home/liweiquan/data/cifar10/cifar-10-png/test.txt   # Imagenet val dataset meta file
        image_reader:
            type: pil
        sampler:
            type: distributed
        transforms:
            type: ONECROP

        evaluator:
            type: imagenet
            kwargs:
                topk: [ 1, 5 ]

saver:                                # saving or loading details
    print_freq: 196                    # frequence of printing logger
    val_freq: 980                    # frequence of evaluating during training
    save_many: False                   # whether to save checkpoints after every evaluation
    pretrain: {}                    # pretrain model details
      #  path: /home/liweiquan/fast-nnclr-4/fast-nnclr/ssl_exp/fast_moco/fastmoco_e100/checkpoints/ckpt_29400_e150.pth

    #     ignore:                     # ignore keys in checkpoints
    #         key:                    # if training from scratch, pop 'optimzier' and 'last_iter'
    #             - optimizer         # if resuming from ckpt, DO NOT pop them
    #             - last_iter
    #         model:                  # ignore modules in model
    #             - module.fc.weight  # if training with different number of classes, pop the keys
    #             - module.fc.bias    # of last fully-connected layers
